{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26920fd4",
   "metadata": {},
   "source": [
    "# PROJET DE STATISTIQUE DES RISQUES EXTREMES\n",
    "\n",
    "Réalisé par : *Mariane ALAPINI & Céleste NENEHIDINI*\n",
    "\n",
    "Sous la supervision de  : **Nicolas JEANNELLE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a52dfe",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec1e83",
   "metadata": {},
   "source": [
    "Notre objectif sur ce projet est de tester plusieurs méthodes d'estimation de la Value-at-Risk (VaR) pour un portefeuille donné. La VaR est une mesure synthétique du risque de marché. Elle est définie comme la perte potentielle maximale que peut subir un portefeuille pour un niveau de confiance donné sur un horizon fixé.\n",
    "\n",
    "Mathématiquement, soit $F$ la fonction de répartition de la variable aléatoire $R$ des rendements de périodicité $h$ du portefeuille. La VaR de niveau de confiance $α$ à horizon d’investissement $h$ est alors définie par :\n",
    "\n",
    "$$\n",
    "\\mathrm{VaR}_h(\\alpha) = F^{-1}(1 - \\alpha)\n",
    "$$\n",
    "\n",
    "On adopte dans ce cas une vision **rentabilité** où $R$ est la distribution des rendements. On peut aussi adopter une vision **risque** en considérant $P = −R$, la distribution des pertes et\n",
    "dans ce cas :\n",
    "\n",
    "$$\n",
    "\\mathrm{VaR}_h(\\alpha) = G^{-1}(\\alpha)\n",
    "$$\n",
    "\n",
    "avec $G$ la fonction de répartition de $P$.\n",
    "\n",
    "Dans la suite, sauf mention contraire, nous raisonnerons d'un point de vue **risque**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c47fb23",
   "metadata": {},
   "source": [
    "## 0. Analyses préliminaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46edf2",
   "metadata": {},
   "source": [
    "**Importation des bibliothèques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e8211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import binomtest\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import gamma\n",
    "from scipy.stats import gumbel_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5b0b4",
   "metadata": {},
   "source": [
    "**a. Importation des données**\n",
    "\n",
    "Pour ce projet, nous avons choisi d'étudier le cours du **S&P 500**, dans sa version ETF. Le S&P 500 (Standard & Poor’s 500) est l’un des indices boursiers les plus suivis au monde. Il regroupe 500 des plus grandes entreprises cotées aux États-Unis, sélectionnées selon des critères de capitalisation boursière, de liquidité et de représentativité sectorielle. L’indice est pondéré par la capitalisation boursière, ce qui signifie que les entreprises les plus importantes (comme Apple, Microsoft ou Amazon) ont un poids plus élevé dans son évolution. De ce fait, le S&P 500 est généralement considéré comme un baromètre de la performance globale du marché actions américain, et plus largement de l’économie américaine.\n",
    "\n",
    "Plutôt que d’analyser directement la valeur théorique de l’indice, nous nous intéressons ici à sa version ETF (Exchange Traded Fund) car elle est directement observable et négociable, avec des données de prix, de volumes et de rendements disponibles. Nous avons considéré les données disponibles sur Yahoo Finance pour les dix dernières années."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06095b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER = \"SPY\"\n",
    "\n",
    "df = yf.download(TICKER, period=\"10y\", interval=\"1d\", auto_adjust=False)\n",
    "\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "last_close = df[\"Close\"].iloc[-1]\n",
    "print(f\"Dernier valeur à la clôture du {TICKER}: {last_close:.2f}\")\n",
    "\n",
    "\n",
    "df.to_csv(f\"price_{TICKER}.csv\")\n",
    "print(\"CSV créé:\", f\"price_{TICKER}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bbf4f9",
   "metadata": {},
   "source": [
    "**b. Vérification de la présence de valeurs manquantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47401ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Close\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ea6a97",
   "metadata": {},
   "source": [
    "Nous n'avons pas de valeurs manquantes dans la série des prix à la clotûre. Il n'y a donc aucun traitement particulier à effectuer avant de débuter les analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910faec1",
   "metadata": {},
   "source": [
    "**c. Calcul du log-rendement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n",
    "# Supprimer la première ligne qui est NaN après le calcul des log-rendements\n",
    "df = df.dropna(subset=[\"log_return\"])\n",
    "df[\"log_return\" ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8753f93",
   "metadata": {},
   "source": [
    "**d. Représentation graphique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig, ax1 = plt.subplots(figsize=(13, 6))\n",
    "\n",
    "# Prix à le cloture \n",
    "ax1.plot(\n",
    "    df.index,\n",
    "    df[\"Close\"],\n",
    "    color=\"blue\",\n",
    "    linewidth=2,\n",
    "    label=\"Price\"\n",
    ")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Price (USD)\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "ax1.grid(True)\n",
    "\n",
    "# Log-rendements\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    df.index,\n",
    "    df[\"log_return\"],\n",
    "    color=\"red\",\n",
    "    alpha=0.6,\n",
    "    label=\"Log return\"\n",
    ")\n",
    "ax2.axhline(0, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "ax2.set_ylabel(\"Log return\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "# Legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
    "\n",
    "plt.title(\"SPY – Prix (en bleu) et log-rendements (rouge)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f7114a",
   "metadata": {},
   "source": [
    "**e. Méthode d'identification automatique des périodes de forte volatilité**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1746a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_volatility_clusters(\n",
    "    df,\n",
    "    return_col=\"log_return\",\n",
    "    short_window=60,\n",
    "    long_window=200,\n",
    "    ratio_threshold=1.5,\n",
    "    min_duration=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect volatility clusters based on local vs long-term volatility divergence.\n",
    "\n",
    "    Returns:\n",
    "    - df with volatility measures and cluster flag\n",
    "    - list of (start_date, end_date) clusters\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Short-term and long-term volatility\n",
    "    df[\"vol_short\"] = df[return_col].rolling(short_window, center=False).std()\n",
    "    df[\"vol_long\"] = df[return_col].rolling(long_window, center=True).std()\n",
    "\n",
    "    # Volatility ratio\n",
    "    df[\"vol_ratio\"] = df[\"vol_short\"] / df[\"vol_long\"]\n",
    "\n",
    "    # Initial high-vol flag\n",
    "    df[\"high_vol_flag\"] = df[\"vol_ratio\"] > ratio_threshold\n",
    "\n",
    "    # ---- Extract clusters (bags) ----\n",
    "    clusters = []\n",
    "    start = None\n",
    "    duration = 0\n",
    "\n",
    "    for date, is_high in df[\"high_vol_flag\"].items():\n",
    "        if is_high:\n",
    "            if start is None:\n",
    "                start = date\n",
    "                duration = 1\n",
    "            else:\n",
    "                duration += 1\n",
    "        else:\n",
    "            if start is not None and duration >= min_duration:\n",
    "                clusters.append((start, date))\n",
    "            start = None\n",
    "            duration = 0\n",
    "\n",
    "    # Edge case\n",
    "    if start is not None and duration >= min_duration:\n",
    "        clusters.append((start, df.index[-1]))\n",
    "\n",
    "    return df, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711500fa",
   "metadata": {},
   "source": [
    "Explication de la démarche : à insérer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6571506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la détection des clusters\n",
    "df_clusters, clusters = detect_volatility_clusters(\n",
    "    df,\n",
    "    return_col=\"log_return\",\n",
    "    short_window=60,\n",
    "    long_window=200,\n",
    "    ratio_threshold=1.2,\n",
    "    min_duration=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f544316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns_with_clusters(df, clusters, return_col=\"log_return\", title=None):\n",
    "    \"\"\"\n",
    "    Plot log-returns and highlight detected volatility clusters.\n",
    "    - df: DataFrame indexé par dates\n",
    "    - clusters: liste de tuples (start_date, end_date)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "    # Série de rendements\n",
    "    ax.plot(df.index, df[return_col], linewidth=1, label=return_col)\n",
    "\n",
    "    # Surlignage des clusters (bandes verticales)\n",
    "    for (start, end) in clusters:\n",
    "        ax.axvspan(start, end, alpha=0.25)  # pas de couleur imposée\n",
    "\n",
    "    ax.axhline(0, linewidth=1)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Log-return\")\n",
    "    ax.set_title(title if title else \"Log-returns avec périodes de forte volatilité\")\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_returns_with_clusters(df_clusters, clusters, return_col=\"log_return\",\n",
    "                           title=\"Train: log-returns + clusters de volatilité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674c566",
   "metadata": {},
   "source": [
    "**f. Découpage train/test**\n",
    "\n",
    "On prend une période de forte volatilité d'au moins 60 jours, précédée d'une période de faible volatilité de 250 jours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b521bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple = df[[\"Close\", \"log_return\"]].copy()\n",
    "df_simple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date1 = \"2020-06-11\"\n",
    "split_date2 = \"2025-04-08\"\n",
    "split_date3 = \"2025-07-08\"\n",
    "\n",
    "train = df_simple.loc[\n",
    "    (df_simple.index >= split_date1) &\n",
    "    (df_simple.index <= split_date2)\n",
    "]\n",
    "\n",
    "test = df_simple.loc[\n",
    "    (df_simple.index > split_date2) &\n",
    "    (df_simple.index <= split_date3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3093ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fa801",
   "metadata": {},
   "source": [
    "On a bien deux échantillons de train et de test qui respectent les conditions requises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737b98c",
   "metadata": {},
   "source": [
    "**g. Statistiques descriptives sur les deux échantillons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_data = [train[\"log_return\"], test[\"log_return\"]]\n",
    "labels = ['Train Set', 'Test Set']\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.boxplot(boxplot_data, labels=labels)\n",
    "plt.title('Boxplot of Log Returns: Train vs Test Set')\n",
    "plt.ylabel('Log Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac533f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(train[\"log_return\"], bins=50, color='blue', label='Train Set', kde=True, stat=\"density\", alpha=0.6)\n",
    "sns.histplot(test[\"log_return\"], bins=50, color='orange', label='Test Set', kde=True, stat=\"density\", alpha=0.6)\n",
    "plt.title('Histogram of Log Returns: Train vs Test Set')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b195bd9",
   "metadata": {},
   "source": [
    "La distribution au niveau du test est beaucoup plus étalée et possède des queues plus lourdes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94921158",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af395cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975d2ca",
   "metadata": {},
   "source": [
    "## 1. Calcul de la VAR non paramétrique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f73edf",
   "metadata": {},
   "source": [
    "**a. Fonction calculant la VaR historique d’un ensemble de log-rendements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92603eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_Hist(log_returns, alpha=0.99):\n",
    "    \"\"\"\n",
    "    VaR historique à 1 jour au niveau alpha à partir de log-rendements.\n",
    "    \n",
    "    Paramètres\n",
    "    ----------\n",
    "    log_returns : array-like ou pd.Series\n",
    "        Série de log-rendements (r_t).\n",
    "    alpha : float\n",
    "        Niveau de confiance (ex: 0.99 pour VaR 99%).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    float\n",
    "        VaR (positive), i.e. perte telle que P(L <= VaR) = alpha avec L = -r.\n",
    "    \"\"\"\n",
    "    r = pd.Series(log_returns).dropna()\n",
    "\n",
    "    # Quantile de la queue gauche des rendements\n",
    "    q = np.quantile(r, 1 - alpha)\n",
    "\n",
    "    # VaR en pertes (positive)\n",
    "    return float(-q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61760a56",
   "metadata": {},
   "source": [
    "**b. VaR historique sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e674c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_99 = VaR_Hist(train[\"log_return\"], alpha=0.99)\n",
    "print(VaR_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b146d",
   "metadata": {},
   "source": [
    "**c. Fonction calculant la VaR historique bootstrap d’un ensemble de log-rendements et donnant un IC de niveau alpha_IC de cette VaR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_Hist_Bootstrap(log_returns, alpha=0.99, alpha_IC=0.95, B=2000, random_state=None):\n",
    "    \"\"\"\n",
    "    VaR historique à 1 jour au niveau alpha + intervalle de confiance bootstrap (percentile)\n",
    "    de niveau alpha_IC.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    log_returns : array-like ou pd.Series\n",
    "        Série de log-rendements r_t.\n",
    "    alpha : float\n",
    "        Niveau de confiance de la VaR (ex: 0.99).\n",
    "    alpha_IC : float\n",
    "        Niveau de confiance de l'intervalle (ex: 0.95).\n",
    "    B : int\n",
    "        Nombre de réplications bootstrap.\n",
    "    random_state : int ou None\n",
    "        Graine pour reproductibilité.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    var_hat : float\n",
    "        VaR historique (positive), avec L = -r.\n",
    "    ic : tuple(float, float)\n",
    "        (borne_inf, borne_sup) de l'IC bootstrap percentile.\n",
    "    \"\"\"\n",
    "    r = pd.Series(log_returns).dropna().to_numpy()\n",
    "    n = r.size\n",
    "    if n < 2:\n",
    "        raise ValueError(\"La série doit contenir au moins 2 rendements non-NaN.\")\n",
    "\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Estimateur VaR historique sur l'échantillon\n",
    "    q = np.quantile(r, 1 - alpha)\n",
    "    var_hat = float(-q)\n",
    "\n",
    "    # Bootstrap\n",
    "    vars_boot = np.empty(B, dtype=float)\n",
    "    for b in range(B):\n",
    "        sample = rng.choice(r, size=n, replace=True)\n",
    "        vars_boot[b] = -np.quantile(sample, 1 - alpha)\n",
    "\n",
    "    # IC percentile (central)\n",
    "    tail = (1 - alpha_IC) / 2\n",
    "    lower = float(np.quantile(vars_boot, tail))\n",
    "    upper = float(np.quantile(vars_boot, 1 - tail))\n",
    "\n",
    "    return var_hat, (lower, upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6974cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_99, ic_95 = VaR_Hist_Bootstrap(train[\"log_return\"], alpha=0.99, alpha_IC=0.95, B=500, random_state=42)\n",
    "print(\"VaR 99% :\", var_99)\n",
    "print(\"IC 95% :\", ic_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc0e23",
   "metadata": {},
   "source": [
    "**d. VaR historique bootstrap et l’IC associé à 90% sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48179ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_99, ic_90 = VaR_Hist_Bootstrap(train[\"log_return\"], alpha=0.99, alpha_IC=0.90, B=500, random_state=42)\n",
    "print(\"VaR 99% :\", var_99)\n",
    "print(\"IC 90% :\", ic_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267529f",
   "metadata": {},
   "source": [
    "**e. Nombre d’exceptions sur base de test associées à la VaR historique calculée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d27239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_exceptions(test_returns, VaR):\n",
    "    \"\"\"\n",
    "    Compte le nombre d'exceptions associées à une VaR donnée.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    test_returns : array-like ou pd.Series\n",
    "        Log-rendements de l'échantillon test.\n",
    "    VaR : float\n",
    "        VaR à 1 jour (positive).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    n_exc : int\n",
    "        Nombre d'exceptions.\n",
    "    rate_exc : float\n",
    "        Taux d'exceptions.\n",
    "    \"\"\"\n",
    "    r_test = pd.Series(test_returns).dropna().to_numpy()\n",
    "\n",
    "    exceptions = r_test < -VaR\n",
    "    n_exc = int(exceptions.sum())\n",
    "    rate_exc = n_exc / len(r_test) if len(r_test) > 0 else np.nan\n",
    "\n",
    "    return n_exc, rate_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exc, rate = nb_exceptions(test[\"log_return\"], VaR_99)\n",
    "print(\"Nombre d'exceptions :\", n_exc)\n",
    "print(\"Taux d'exceptions   :\", rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c5abc",
   "metadata": {},
   "source": [
    "**f. Comparaison avec le niveau de risque attendu (IC de binomiale)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe952dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = n_exc\n",
    "n = test.shape[0]\n",
    "p0 = 0.01          # attendu pour VaR 99%\n",
    "alpha_IC = 0.9\n",
    "\n",
    "# IC exact Clopper-Pearson\n",
    "res = binomtest(k, n, p=p0, alternative=\"greater\")\n",
    "ci = res.proportion_ci(confidence_level=alpha_IC, method=\"exact\")\n",
    "\n",
    "print(\"Taux observé:\", k/n)\n",
    "print(f\"IC {alpha_IC:.0%} (Clopper-Pearson): [{ci.low:.4f}, {ci.high:.4f}]\")\n",
    "print(\"p-value :\", res.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cacc0",
   "metadata": {},
   "source": [
    "Le taux attendu sous une VaR 99% est 1%, et 1% n’appartient pas à cet intervalle de confiance calculée. Statistiquement, le taux d’exceptions observé est incompatible avec une VaR 99% bien calibrée. On a beaucoup trop d’exceptions et donc la VaR sous-estime le risque sur le test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a997ab6",
   "metadata": {},
   "source": [
    "## 2. Calcul de la VaR Gaussienne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e5c4a",
   "metadata": {},
   "source": [
    "**a. Fonction calculant la VaR gaussienne d’un ensemble de log-rendements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a648047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_Gauss(log_returns, alpha=0.99):\n",
    "    \"\"\"\n",
    "    VaR gaussienne à 1 jour au niveau alpha.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    log_returns : array-like ou pd.Series\n",
    "        Série de log-rendements.\n",
    "    alpha : float\n",
    "        Niveau de confiance (ex: 0.99).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    float\n",
    "        VaR gaussienne (positive).\n",
    "    \"\"\"\n",
    "    r = pd.Series(log_returns).dropna()\n",
    "\n",
    "    mu = r.mean()\n",
    "    sigma = r.std(ddof=1)\n",
    "\n",
    "    z = norm.ppf(1 - alpha)\n",
    "\n",
    "    VaR = -(mu + sigma * z)\n",
    "    return float(VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbe35a",
   "metadata": {},
   "source": [
    "**b. VaR gaussienne sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c66d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_gauss_99 = VaR_Gauss(train[\"log_return\"], alpha=0.99)\n",
    "print(VaR_gauss_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf20a4f",
   "metadata": {},
   "source": [
    "**c. Validation ex-ante (analyses graphiques, QQ-plot, etc)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_ex_ante_gaussienne(log_returns, alpha=0.99, window_vol=20):\n",
    "    \"\"\"\n",
    "    Validation ex-ante pour l'hypothèse gaussienne sur une série de log-rendements.\n",
    "    - Série de rendements\n",
    "    - Volatilité glissante (clustering)\n",
    "    - Histogramme + fit normal\n",
    "    - QQ-plot vs Normal(0,1) (standardisé)\n",
    "    - Marque le quantile (1-alpha) théorique sous normalité (queue gauche)\n",
    "    \"\"\"\n",
    "    r = pd.Series(log_returns).dropna()\n",
    "    n = len(r)\n",
    "\n",
    "    mu = r.mean()\n",
    "    sigma = r.std(ddof=1)\n",
    "\n",
    "    # standardisation\n",
    "    z = (r - mu) / sigma\n",
    "\n",
    "    # quantile queue gauche (rendements)\n",
    "    q_emp = np.quantile(r, 1 - alpha)\n",
    "    q_norm = mu + sigma * stats.norm.ppf(1 - alpha)\n",
    "\n",
    "    # ---- 1) Rendements (time series) ----\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(r.index, r.values)\n",
    "    plt.axhline(q_emp, linestyle=\"--\")\n",
    "    plt.axhline(q_norm, linestyle=\"--\")\n",
    "    plt.title(f\"Log-rendements (n={n}) et quantile (1-α) | α={alpha}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"log_return\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 2) Volatilité glissante ----\n",
    "    vol_roll = r.rolling(window_vol).std()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(vol_roll.index, vol_roll.values)\n",
    "    plt.title(f\"Volatilité glissante (std sur {window_vol} jours) — détection clustering\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"rolling std\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 3) Histogramme + fit normal ----\n",
    "    plt.figure(figsize=(8, 4.8))\n",
    "    plt.hist(r.values, bins=60, density=True)\n",
    "\n",
    "    xs = np.linspace(r.min(), r.max(), 400)\n",
    "    plt.plot(xs, stats.norm.pdf(xs, loc=mu, scale=sigma))\n",
    "    plt.axvline(q_emp, linestyle=\"--\", linewidth=1)\n",
    "    plt.axvline(q_norm, linestyle=\"--\", linewidth=1)\n",
    "    plt.title(\"Histogramme des rendements + densité normale ajustée\")\n",
    "    plt.xlabel(\"log_return\")\n",
    "    plt.ylabel(\"densité\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 4) QQ-plot (standardisé) ----\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    stats.probplot(z.values, dist=\"norm\", plot=plt)\n",
    "    plt.title(\"QQ-plot des rendements standardisés vs N(0,1)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- 5) Statistiques simples (queue / asymétrie) ----\n",
    "    skew = stats.skew(r.values, bias=False)\n",
    "    kurt = stats.kurtosis(r.values, fisher=True, bias=False)  # excess kurtosis\n",
    "\n",
    "    print(f\"mu = {mu:.6f}, sigma = {sigma:.6f}\")\n",
    "    print(f\"Quantile empirique (1-α) : {q_emp:.6f}\")\n",
    "    print(f\"Quantile normal ajusté (1-α) : {q_norm:.6f}\")\n",
    "    print(f\"Asymétrie (skewness) : {skew:.3f}\")\n",
    "    print(f\"Kurtosis excédentaire : {kurt:.3f}  (0 si normal)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ex_ante_gaussienne(train[\"log_return\"], alpha=0.99, window_vol=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea95525",
   "metadata": {},
   "source": [
    "**d. VaR gaussienne à 10j par la méthode du scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3268ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VaR_gauss_10j = VaR_gauss_99 * np.sqrt(10)\n",
    "print(VaR_gauss_10j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b8177",
   "metadata": {},
   "source": [
    "**e. VaR gaussienne à 10j par méthode de diffusion d’un actif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735d03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_gaussienne_diffusion(S0, mu, sigma, alpha=0.99, n_sims=200000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T = 10  # jours\n",
    "    Z = rng.standard_normal(n_sims)\n",
    "\n",
    "    # GBM exact sur horizon T\n",
    "    ST = S0 * np.exp((mu - 0.5 * sigma**2) * T + sigma * np.sqrt(T) * Z)\n",
    "\n",
    "    # perte (positive si baisse)\n",
    "    loss = S0 - ST\n",
    "\n",
    "    # VaR = quantile alpha des pertes\n",
    "    return float(np.quantile(loss, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447df706",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = train[\"Close\"].iloc[-1]\n",
    "mu = train[\"log_return\"].mean()\n",
    "sigma = train[\"log_return\"].std(ddof=1)\n",
    "\n",
    "VaR_10j_99 = var_gaussienne_diffusion(S0, mu, sigma, alpha=0.99, n_sims=20000, seed=42)\n",
    "print(VaR_10j_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52e95f",
   "metadata": {},
   "source": [
    "Calcul précédent à revoir !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13841cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_gaussienne_10j_rendement(mu, sigma, alpha=0.99):\n",
    "    T = 10\n",
    "    z = norm.ppf(1 - alpha)  # négatif\n",
    "    return float(-(mu*T + sigma*np.sqrt(T)*z))  # VaR en % (positive)\n",
    "\n",
    "VaR_10j_99 = var_gaussienne_10j_rendement(mu, sigma, alpha=0.99)\n",
    "print(VaR_10j_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca7ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = float(train[\"Close\"].iloc[-1])\n",
    "mu = float(train[\"log_return\"].mean())\n",
    "sigma = float(train[\"log_return\"].std(ddof=1))\n",
    "\n",
    "VaR_10j_99 = var_gaussienne_diffusion(S0, mu, sigma, alpha=0.99, n_sims=20000, seed=42)\n",
    "print(VaR_10j_99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba869a1",
   "metadata": {},
   "source": [
    "## 3. VaR skew-Student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdff122",
   "metadata": {},
   "source": [
    "**a. Estimation des paramètres d’une loi de Skew Student par maximum de vraisemblance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c122643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Skewed Student-t de Hansen (1994)\n",
    "# Paramètres:\n",
    "#   mu    : localisation\n",
    "#   sigma : échelle (>0)\n",
    "#   nu    : ddl (>2)\n",
    "#   lam   : skewness dans (-1, 1)\n",
    "# ----------------------------\n",
    "\n",
    "def _hansen_constants(nu, lam):\n",
    "    \"\"\"\n",
    "    Calcule a, b, c de Hansen (1994) pour la skewed t.\n",
    "    \"\"\"\n",
    "    c = gamma((nu + 1) / 2) / (np.sqrt(np.pi * (nu - 2)) * gamma(nu / 2))\n",
    "    a = 4 * lam * c * (nu - 2) / (nu - 1)\n",
    "    b = np.sqrt(1 + 3 * lam**2 - a**2)\n",
    "    return a, b, c\n",
    "\n",
    "def skewt_pdf(x, mu, sigma, nu, lam):\n",
    "    \"\"\"\n",
    "    PDF de la skewed t de Hansen (1994) évaluée en x (vectorisé).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    if sigma <= 0 or nu <= 2 or not (-1 < lam < 1):\n",
    "        return np.zeros_like(x, dtype=float)\n",
    "\n",
    "    a, b, c = _hansen_constants(nu, lam)\n",
    "    z = (x - mu) / sigma\n",
    "\n",
    "    # seuil de bascule\n",
    "    thresh = -a / b\n",
    "\n",
    "    left = z < thresh\n",
    "    right = ~left\n",
    "\n",
    "    pdf = np.empty_like(z, dtype=float)\n",
    "\n",
    "    # partie gauche\n",
    "    denom_L = (1 - lam)\n",
    "    arg_L = (b * z[left] + a) / denom_L\n",
    "    pdf[left] = (b * c / sigma) * (1 + (arg_L**2) / (nu - 2)) ** (-(nu + 1) / 2)\n",
    "\n",
    "    # partie droite\n",
    "    denom_R = (1 + lam)\n",
    "    arg_R = (b * z[right] + a) / denom_R\n",
    "    pdf[right] = (b * c / sigma) * (1 + (arg_R**2) / (nu - 2)) ** (-(nu + 1) / 2)\n",
    "\n",
    "    return pdf\n",
    "\n",
    "def skewt_loglik(params, x):\n",
    "    \"\"\"\n",
    "    Log-vraisemblance (somme des log pdf).\n",
    "    params = (mu, log_sigma, log_nu_minus2, atanh_lam)\n",
    "    \"\"\"\n",
    "    mu, log_sigma, log_nu_m2, atanh_lam = params\n",
    "    sigma = np.exp(log_sigma)\n",
    "    nu = 2.0 + np.exp(log_nu_m2)      # garantit nu > 2\n",
    "    lam = np.tanh(atanh_lam)          # garantit lam in (-1, 1)\n",
    "\n",
    "    pdf = skewt_pdf(x, mu, sigma, nu, lam)\n",
    "    # sécurité numérique\n",
    "    eps = 1e-14\n",
    "    return np.sum(np.log(pdf + eps))\n",
    "\n",
    "def fit_skewt_mle(x, mu0=None, sigma0=None, nu0=8.0, lam0=0.0):\n",
    "    \"\"\"\n",
    "    Estime (mu, sigma, nu, lam) par MLE avec scipy.optimize.minimize.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "\n",
    "    if mu0 is None:\n",
    "        mu0 = np.mean(x)\n",
    "    if sigma0 is None:\n",
    "        sigma0 = np.std(x, ddof=1)\n",
    "\n",
    "    # paramètres transformés initiaux\n",
    "    p0 = np.array([\n",
    "        mu0,\n",
    "        np.log(max(sigma0, 1e-8)),\n",
    "        np.log(max(nu0 - 2.0, 1e-6)),\n",
    "        np.arctanh(np.clip(lam0, -0.999, 0.999))\n",
    "    ], dtype=float)\n",
    "\n",
    "    # on maximise loglik => on minimise -loglik\n",
    "    def objective(p):\n",
    "        return -skewt_loglik(p, x)\n",
    "\n",
    "    res = minimize(objective, p0, method=\"L-BFGS-B\")\n",
    "\n",
    "    mu_hat, log_sigma_hat, log_nu_m2_hat, atanh_lam_hat = res.x\n",
    "    sigma_hat = np.exp(log_sigma_hat)\n",
    "    nu_hat = 2.0 + np.exp(log_nu_m2_hat)\n",
    "    lam_hat = np.tanh(atanh_lam_hat)\n",
    "\n",
    "    out = {\n",
    "        \"mu\": mu_hat,\n",
    "        \"sigma\": sigma_hat,\n",
    "        \"nu\": nu_hat,\n",
    "        \"lam\": lam_hat,\n",
    "        \"success\": res.success,\n",
    "        \"message\": res.message,\n",
    "        \"fun\": res.fun,  # valeur min de -loglik\n",
    "        \"loglik\": -res.fun\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b7f8a",
   "metadata": {},
   "source": [
    "**b. Paramètres de loi Skew Student sur base d’apprentissage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit skew-t\n",
    "skewt_fit = fit_skewt_mle(train['log_return'], nu0=8.0, lam0=0.0)\n",
    "print(\"Skew-t MLE:\", skewt_fit)\n",
    "\n",
    "# Fit gaussien (MLE fermé)\n",
    "mu_g, sigma_g = np.mean(train['log_return']), np.std(train['log_return'], ddof=1)\n",
    "print(\"Gaussian:\", {\"mu\": mu_g, \"sigma\": sigma_g})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f881c",
   "metadata": {},
   "source": [
    "**c. Validation ex-ante par QQ-plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_skewt_hansen(n, mu, sigma, nu, lam, rng=None):\n",
    "    \"\"\"\n",
    "    Génération par transformation:\n",
    "    - On génère une Student-t symétrique t_nu\n",
    "    - On applique la transformation piecewise de Hansen sur z\n",
    "    - puis x = mu + sigma*z\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    a, b, c = _hansen_constants(nu, lam)\n",
    "\n",
    "    # Student t symétrique standard (moyenne 0)\n",
    "    # On génère via ratio normal / sqrt(chi2/nu)\n",
    "    z0 = rng.standard_normal(n)\n",
    "    u  = rng.chisquare(df=nu, size=n)\n",
    "    t  = z0 / np.sqrt(u / nu)\n",
    "\n",
    "    # On applique la transformation inverse de la définition piecewise\n",
    "    # Hansen définit la densité en fonction de z, avec seuil -a/b\n",
    "    # Ici on transforme t -> z en inversant l'argument piecewise.\n",
    "    # On utilise la même séparation via probabilité implicite:\n",
    "    # plus simple: on utilise la construction équivalente:\n",
    "    #   z = ((1-lam)*t - a)/b si t < 0\n",
    "    #   z = ((1+lam)*t - a)/b si t >= 0\n",
    "    # Cette construction est couramment utilisée avec cette paramétrisation.\n",
    "    z = np.empty_like(t)\n",
    "    neg = t < 0\n",
    "    z[neg]  = ((1 - lam) * t[neg]  - a) / b\n",
    "    z[~neg] = ((1 + lam) * t[~neg] - a) / b\n",
    "\n",
    "    return mu + sigma * z\n",
    "\n",
    "def qqplot_gaussian(x, mu, sigma, title=\"QQ-plot Gaussian\"):\n",
    "    x = np.asarray(x)\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x_sorted)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "    theo = norm.ppf(p, loc=mu, scale=sigma)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(theo, x_sorted, marker='o', linestyle='none')\n",
    "    # droite 45° (référence)\n",
    "    mn = min(theo.min(), x_sorted.min())\n",
    "    mx = max(theo.max(), x_sorted.max())\n",
    "    plt.plot([mn, mx], [mn, mx])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Quantiles théoriques\")\n",
    "    plt.ylabel(\"Quantiles empiriques\")\n",
    "    plt.show()\n",
    "\n",
    "def qqplot_skewt_sim(x, fit, nsim=200000, seed=123, title=\"QQ-plot Skew-t (simulation)\"):\n",
    "    x = np.asarray(x)\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x_sorted)\n",
    "    p = (np.arange(1, n+1) - 0.5) / n\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sim = sample_skewt_hansen(nsim, fit[\"mu\"], fit[\"sigma\"], fit[\"nu\"], fit[\"lam\"], rng=rng)\n",
    "    sim_sorted = np.sort(sim)\n",
    "\n",
    "    # quantiles simulés aux mêmes probabilités\n",
    "    idx = np.clip((p * (nsim - 1)).astype(int), 0, nsim - 1)\n",
    "    theo = sim_sorted[idx]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(theo, x_sorted, marker='o', linestyle='none')\n",
    "    mn = min(theo.min(), x_sorted.min())\n",
    "    mx = max(theo.max(), x_sorted.max())\n",
    "    plt.plot([mn, mx], [mn, mx])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Quantiles théoriques (skew-t simulée)\")\n",
    "    plt.ylabel(\"Quantiles empiriques\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ-plots ex-ante sur test\n",
    "qqplot_gaussian(test['log_return'], mu_g, sigma_g, title=\"QQ-plot ex-ante (Gaussian) - test\")\n",
    "qqplot_skewt_sim(test['log_return'], skewt_fit, nsim=300000, seed=1, title=\"QQ-plot ex-ante (Skew-t Hansen) - test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82724ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ-plots ex-ante sur train\n",
    "qqplot_gaussian(train['log_return'], mu_g, sigma_g, title=\"QQ-plot ex-ante (Gaussian) - train\")\n",
    "qqplot_skewt_sim(train['log_return'], skewt_fit, nsim=300000, seed=1, title=\"QQ-plot ex-ante (Skew-t Hansen) - train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee72ae",
   "metadata": {},
   "source": [
    "**d. Comparaison de la qualité de fit entre loi gaussienne et loi de skew Student par analyse graphique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35524cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_comparison(x, mu_g, sigma_g, skewt_fit, bins=60, title=\"Fit comparison (train)\"):\n",
    "    x = np.asarray(x)\n",
    "    grid = np.linspace(np.percentile(x, 0.5), np.percentile(x, 99.5), 800)\n",
    "\n",
    "    pdf_g = norm.pdf(grid, loc=mu_g, scale=sigma_g)\n",
    "    pdf_s = skewt_pdf(grid, skewt_fit[\"mu\"], skewt_fit[\"sigma\"], skewt_fit[\"nu\"], skewt_fit[\"lam\"])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(x, bins=bins, density=True, alpha=0.4)\n",
    "    plt.plot(grid, pdf_g, label=\"Gaussian\")\n",
    "    plt.plot(grid, pdf_s, label=\"Skew-t (Hansen)\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Rendement\")\n",
    "    plt.ylabel(\"Densité\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit_comparison(train['log_return'], mu_g, sigma_g, skewt_fit, title=\"Fit comparison - train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bde2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit_comparison(test['log_return'], mu_g, sigma_g, skewt_fit, title=\"Fit comparison - test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bcb71",
   "metadata": {},
   "source": [
    "**e. VaR Skew Student sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44faa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_skewt_99_train(skewt_fit, nsim=2_000_000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sim = sample_skewt_hansen(nsim, skewt_fit[\"mu\"], skewt_fit[\"sigma\"], skewt_fit[\"nu\"], skewt_fit[\"lam\"], rng=rng)\n",
    "    q01 = np.quantile(sim, 0.01)      # 1% quantile (rendements)\n",
    "    VaR99 = -q01                      # VaR positive (perte)\n",
    "    return VaR99, q01\n",
    "\n",
    "VaR99_skewt, q01_skewt = var_skewt_99_train(skewt_fit, nsim=1_000_000, seed=7)\n",
    "print(\"Skew-t: q01 (returns) =\", q01_skewt, \"| VaR99 (loss) =\", VaR99_skewt)\n",
    "\n",
    "# Pour comparaison: VaR gaussienne\n",
    "q01_gauss = norm.ppf(0.01, loc=mu_g, scale=sigma_g)\n",
    "VaR99_gauss = -q01_gauss\n",
    "print(\"Gaussian: q01 (returns) =\", q01_gauss, \"| VaR99 (loss) =\", VaR99_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab05fd8",
   "metadata": {},
   "source": [
    "## 4. Protocole de recalibrage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9c9740",
   "metadata": {},
   "source": [
    "## 5. VaR TVE : Approche Maxima par bloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c752d305",
   "metadata": {},
   "source": [
    "**a. Détermination d'une taille de bloc s et construction d'un échantillon de maxima sur la base d’apprentissage**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9330427",
   "metadata": {},
   "source": [
    "On doit d'abord convertir les données car en TVE classique, on modélise des maxima. Or en gestion du risque, ce sont les pertes extrêmes (rendements très négatifs) qui nous intéressent. On transforme donc la série de rendements en pertes positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898474dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_returns_to_losses(df, return_col=\"log_return\", loss_col=\"loss\"):\n",
    "    \"\"\"\n",
    "    Convertit les rendements en pertes positives\n",
    "    pour application de la théorie des valeurs extrêmes.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[loss_col] = -df[return_col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_losses = convert_returns_to_losses(train, return_col=\"log_return\")\n",
    "df_train_losses[[ \"log_return\", \"loss\" ]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9011e",
   "metadata": {},
   "source": [
    "Nous prendrons une taille de blocs mensuelle, s=21 jours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1488a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_maxima(df, loss_col=\"loss\", block_size=21):\n",
    "    \"\"\"\n",
    "    Construit l'échantillon des maxima par blocs (Block Maxima).\n",
    "\n",
    "    Returns:\n",
    "    - Series des maxima de blocs\n",
    "    \"\"\"\n",
    "    losses = df[loss_col].values\n",
    "    n = len(losses)\n",
    "\n",
    "    # Nombre de blocs complets\n",
    "    n_blocks = n // block_size\n",
    "\n",
    "    # On tronque la série pour avoir des blocs complets\n",
    "    losses_trimmed = losses[: n_blocks * block_size]\n",
    "\n",
    "    # Reshape (blocs x taille_bloc)\n",
    "    blocks = losses_trimmed.reshape(n_blocks, block_size)\n",
    "\n",
    "    # Maxima par bloc\n",
    "    block_max = blocks.max(axis=1)\n",
    "\n",
    "    return pd.Series(block_max, name=\"block_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_maxima_train = block_maxima(\n",
    "    df_train_losses,\n",
    "    loss_col=\"loss\",\n",
    "    block_size=21\n",
    ")\n",
    "\n",
    "print(\"Nombre de maxima:\", len(block_maxima_train))\n",
    "block_maxima_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469211d",
   "metadata": {},
   "source": [
    "**b. Gumbel plot pour juger de l’hypothèse ξ=0 (i.e. GEV vs EV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b960674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_plot(block_maxima):\n",
    "    \"\"\"\n",
    "    Gumbel plot pour tester l'hypothèse xi = 0 (EV vs GEV).\n",
    "    \"\"\"\n",
    "    x = np.sort(np.asarray(block_maxima))\n",
    "    n = len(x)\n",
    "\n",
    "    # Positions empiriques (plotting positions)\n",
    "    p = (np.arange(1, n + 1) - 0.5) / n\n",
    "\n",
    "    # Variable réduite de Gumbel\n",
    "    y = -np.log(-np.log(p))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y, x, s=25)\n",
    "    \n",
    "    # Ajustement linéaire (référence visuelle)\n",
    "    a, b = np.polyfit(y, x, 1)\n",
    "    #plt.plot(y, a * y + b, linewidth=1)\n",
    "\n",
    "    plt.xlabel(r\"$-\\log(-\\log(\\hat F(x)))$\")\n",
    "    plt.ylabel(\"Maxima de blocs (pertes)\")\n",
    "    plt.title(\"Gumbel plot des maxima de blocs\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat, b_hat = gumbel_plot(block_maxima_train)\n",
    "\n",
    "print(\"Pente estimée (sigma approx.) :\", a_hat)\n",
    "print(\"Ordonnée à l'origine (mu approx.) :\", b_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb00a24",
   "metadata": {},
   "source": [
    "Le gumbel plot donne une représentation assez linéaire, l'hypothèse ξ=0 est très plausible. On va donc partir sur une estimation de la loi de Gumbel EV qui est donc pertinente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578ab66",
   "metadata": {},
   "source": [
    "**c. Estimation des paramètres de loi EV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(block_maxima_train)\n",
    "loc_hat, scale_hat = gumbel_r.fit(x)\n",
    "\n",
    "print(\"Estimation Gumbel (EV, xi=0)\")\n",
    "print(\"loc (mu)   =\", loc_hat)\n",
    "print(\"scale (beta) =\", scale_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b804a",
   "metadata": {},
   "source": [
    "**d. Validation ex-ante (analyses graphiques, QQ-plot, etc**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4962b",
   "metadata": {},
   "source": [
    "Ajustement avec la densité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c31c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.linspace(x.min(), x.max(), 500)\n",
    "pdf_fit = gumbel_r.pdf(grid, loc=loc_hat, scale=scale_hat)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(x, bins=15, density=True, alpha=0.4, label=\"Maxima empiriques\")\n",
    "plt.plot(grid, pdf_fit, label=\"Gumbel ajustée\")\n",
    "plt.title(\"Ajustement Gumbel sur les maxima de blocs\")\n",
    "plt.xlabel(\"Maxima de blocs (pertes)\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011808ac",
   "metadata": {},
   "source": [
    "QQ-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qqplot_gumbel(block_maxima, loc, scale, title=None):\n",
    "    \"\"\"\n",
    "    QQ-plot des maxima de blocs vs loi de Gumbel ajustée.\n",
    "    \n",
    "    block_maxima : array-like, maxima de blocs\n",
    "    loc, scale   : paramètres MLE de la loi de Gumbel\n",
    "    \"\"\"\n",
    "    x = np.sort(np.asarray(block_maxima))\n",
    "    n = len(x)\n",
    "\n",
    "    # Positions empiriques\n",
    "    p = (np.arange(1, n + 1) - 0.5) / n\n",
    "\n",
    "    # Quantiles théoriques Gumbel\n",
    "    q_theo = gumbel_r.ppf(p, loc=loc, scale=scale)\n",
    "\n",
    "    # QQ-plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(q_theo, x, s=25)\n",
    "    \n",
    "    # Droite 45°\n",
    "    mn = min(q_theo.min(), x.min())\n",
    "    mx = max(q_theo.max(), x.max())\n",
    "    plt.plot([mn, mx], [mn, mx], linewidth=1)\n",
    "\n",
    "    plt.xlabel(\"Quantiles théoriques Gumbel\")\n",
    "    plt.ylabel(\"Quantiles empiriques (maxima de blocs)\")\n",
    "    plt.title(title if title else \"QQ-plot Gumbel\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ef0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot_gumbel(\n",
    "    block_maxima_train,\n",
    "    loc=loc_hat,\n",
    "    scale=scale_hat,\n",
    "    title=\"QQ-plot des maxima vs Gumbel ajustée\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c75ab",
   "metadata": {},
   "source": [
    "**e. Calcul de la VaR TVE par MB sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff5efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.99 \n",
    "s = 21\n",
    "\n",
    "# Niveau à utiliser pour le quantile des maxima\n",
    "p_block = alpha ** s\n",
    "\n",
    "# VaR TVE (sur pertes journalières)\n",
    "VaR99_loss = gumbel_r.ppf(p_block, loc=loc_hat, scale=scale_hat)\n",
    "\n",
    "print(\"p_block =\", p_block)\n",
    "print(\"VaR 99% (pertes) =\", VaR99_loss)\n",
    "\n",
    "# VaR en rendement (seuil sur r_t)\n",
    "VaR99_return = -VaR99_loss\n",
    "print(\"Seuil VaR 99% en rendements =\", VaR99_return)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f18de",
   "metadata": {},
   "source": [
    "**f. Protocole permettant de trouver une taille de bloc optimale**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cdc922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "108e6af9",
   "metadata": {},
   "source": [
    "## 6. VaR TVE : Approche Peak over threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a389431",
   "metadata": {},
   "source": [
    "**a. Fonction permettant d’obtenir le mean excess plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca49bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_excess_plot(\n",
    "    losses,\n",
    "    min_exceedances=50,\n",
    "    force_positive_u=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Mean Excess Plot en testant tous les seuils u = x_(i),\n",
    "    tout en tronquant à droite via une contrainte N_u >= min_exceedances.\n",
    "    \"\"\"\n",
    "    x = np.asarray(losses)\n",
    "    x = x[np.isfinite(x)]\n",
    "\n",
    "    if force_positive_u:\n",
    "        x = x[x >= 0]\n",
    "\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x_sorted)\n",
    "\n",
    "    u_list, e_list, Nu_list = [], [], []\n",
    "\n",
    "    # On teste chaque valeur comme seuil (sauf les toutes dernières)\n",
    "    # et on tronque à droite en imposant N_u >= min_exceedances\n",
    "    for i in range(n - min_exceedances):\n",
    "        u = x_sorted[i]\n",
    "        exceedances = x_sorted[i+1:]      # car x_sorted > u (si pas de doublons)\n",
    "        exceedances = exceedances[exceedances > u]  # robuste aux doublons\n",
    "        Nu = len(exceedances)\n",
    "\n",
    "        if Nu < min_exceedances:\n",
    "            break  # à partir de là, Nu ne fera que diminuer => tronquage à droite\n",
    "\n",
    "        u_list.append(u)\n",
    "        e_list.append(np.mean(exceedances - u))\n",
    "        Nu_list.append(Nu)\n",
    "\n",
    "    u_list = np.array(u_list)\n",
    "    e_list = np.array(e_list)\n",
    "    Nu_list = np.array(Nu_list)\n",
    "\n",
    "    # Plot + diagnostic du nombre d'excès\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9, 7), sharex=True)\n",
    "\n",
    "    axes[0].plot(u_list, e_list)\n",
    "    axes[0].set_ylabel(\"Excès moyen e(u)\")\n",
    "    axes[0].set_title(\"Mean Excess Plot — tous seuils, tronqué à droite (N_u min)\")\n",
    "\n",
    "    axes[1].plot(u_list, Nu_list)\n",
    "    axes[1].set_xlabel(\"Seuil u\")\n",
    "    axes[1].set_ylabel(\"Nombre d'excès N_u\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return u_list, e_list, Nu_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef22511",
   "metadata": {},
   "source": [
    "**b. Détermination de u par analyse graphique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99330ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_vals, e_vals, Nu_vals = mean_excess_plot(\n",
    "    df_train_losses[\"loss\"],\n",
    "    min_exceedances=50,\n",
    "    force_positive_u=True\n",
    ")\n",
    "\n",
    "print(\"Points tracés:\", len(u_vals))\n",
    "print(\"N_u min/max:\", Nu_vals.min(), Nu_vals.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ea6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_threshold_by_target_exceedances(u_vals, Nu_vals, target=100):\n",
    "    idx = np.argmin(np.abs(Nu_vals - target))\n",
    "    return u_vals[idx], Nu_vals[idx]\n",
    "\n",
    "u_star, Nu_star = pick_threshold_by_target_exceedances(u_vals, Nu_vals, target=100)\n",
    "print(\"Seuil choisi u* =\", u_star)\n",
    "print(\"Nombre d'excès associé N_u =\", Nu_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(u_vals, e_vals, linewidth=1)\n",
    "plt.axvline(u_star, linestyle=\"--\", linewidth=1, label=f\"u* (N_u≈{Nu_star})\")\n",
    "plt.xlabel(\"Seuil u\")\n",
    "plt.ylabel(\"Excès moyen e(u)\")\n",
    "plt.title(\"Mean Excess Plot — seuil sélectionné\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d35da4",
   "metadata": {},
   "source": [
    "**c. Estimation des paramètres de loi GPD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecced44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_exceedances(losses, u):\n",
    "    \"\"\"\n",
    "    losses : array-like (pertes X = -r)\n",
    "    u      : seuil\n",
    "    Retourne :\n",
    "      - exc : les excès Y = X - u conditionnellement à X > u\n",
    "      - Nu  : nombre d'excès\n",
    "    \"\"\"\n",
    "    x = np.asarray(losses)\n",
    "    x = x[np.isfinite(x)]\n",
    "    x_exc = x[x > u]\n",
    "    exc = x_exc - u\n",
    "    return exc, len(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29c61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "exc, Nu = build_exceedances(df_train_losses[\"loss\"], u_star)\n",
    "print(\"Seuil u =\", u_star)\n",
    "print(\"Nombre d'excès Nu =\", Nu)\n",
    "print(\"Excès (head):\", exc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f8a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import genpareto\n",
    "\n",
    "def fit_gpd_mle(exceedances):\n",
    "    \"\"\"\n",
    "    Estime les paramètres GPD (xi, beta) sur des excès >= 0.\n",
    "    Retourne un dict avec xi (shape) et beta (scale).\n",
    "    \"\"\"\n",
    "    y = np.asarray(exceedances)\n",
    "    y = y[np.isfinite(y)]\n",
    "    y = y[y >= 0]\n",
    "\n",
    "    # Fit MLE: genpareto(c=xi, loc=0, scale=beta)\n",
    "    xi_hat, loc_hat, beta_hat = genpareto.fit(y, floc=0)\n",
    "\n",
    "    return {\"xi\": xi_hat, \"beta\": beta_hat, \"loc\": loc_hat}\n",
    "\n",
    "# Exemple d'utilisation\n",
    "gpd_fit = fit_gpd_mle(exc)\n",
    "print(\"GPD MLE:\", gpd_fit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bebb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = exc\n",
    "grid = np.linspace(0, np.max(y), 400)\n",
    "pdf = genpareto.pdf(grid, c=gpd_fit[\"xi\"], loc=0, scale=gpd_fit[\"beta\"])\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(y, bins=30, density=True, alpha=0.4, label=\"Excès empiriques\")\n",
    "plt.plot(grid, pdf, label=\"GPD ajustée\")\n",
    "plt.title(\"Ajustement GPD sur les excès (POT)\")\n",
    "plt.xlabel(\"Excès y = X - u\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import genpareto\n",
    "\n",
    "def qqplot_gpd(exceedances, xi, beta, title=None):\n",
    "    \"\"\"\n",
    "    QQ-plot des excès vs loi GPD ajustée.\n",
    "    \n",
    "    exceedances : array-like, Y = X - u (>= 0)\n",
    "    xi          : paramètre de forme GPD\n",
    "    beta        : paramètre d'échelle GPD\n",
    "    \"\"\"\n",
    "    y = np.asarray(exceedances)\n",
    "    y = y[np.isfinite(y)]\n",
    "    y = y[y >= 0]\n",
    "\n",
    "    # Quantiles empiriques\n",
    "    y_sorted = np.sort(y)\n",
    "    n = len(y_sorted)\n",
    "    p = (np.arange(1, n + 1) - 0.5) / n\n",
    "\n",
    "    # Quantiles théoriques GPD\n",
    "    q_theo = genpareto.ppf(p, c=xi, loc=0, scale=beta)\n",
    "\n",
    "    # QQ-plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(q_theo, y_sorted, s=25)\n",
    "    \n",
    "    # Diagonale\n",
    "    mn = min(q_theo.min(), y_sorted.min())\n",
    "    mx = max(q_theo.max(), y_sorted.max())\n",
    "    plt.plot([mn, mx], [mn, mx], linewidth=1)\n",
    "\n",
    "    plt.xlabel(\"Quantiles théoriques GPD\")\n",
    "    plt.ylabel(\"Quantiles empiriques (excès)\")\n",
    "    plt.title(title if title else \"QQ-plot GPD (excès POT)\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86419fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot_gpd(\n",
    "    exc,\n",
    "    xi=gpd_fit[\"xi\"],\n",
    "    beta=gpd_fit[\"beta\"],\n",
    "    title=\"QQ-plot des excès vs GPD ajustée\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efe4ae7",
   "metadata": {},
   "source": [
    "**e. VaR TVE par PoT sur base d’apprentissage pour alpha = 99%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c63aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_pot_gpd(alpha, u, xi, beta, Nu, n):\n",
    "    \"\"\"\n",
    "    VaR POT basée sur une GPD ajustée sur les excès (X > u).\n",
    "    alpha : niveau (ex 0.99)\n",
    "    u     : seuil\n",
    "    xi    : shape GPD\n",
    "    beta  : scale GPD\n",
    "    Nu    : nombre d'excès\n",
    "    n     : taille totale échantillon\n",
    "    \"\"\"\n",
    "    pu = Nu / n  # fréquence d'excès\n",
    "\n",
    "    if xi != 0:\n",
    "        term = ((1 - alpha) / pu) ** (-xi) - 1\n",
    "        return u + (beta / xi) * term\n",
    "    else:\n",
    "        return u + beta * np.log(pu / (1 - alpha))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exemple d'utilisation ---\n",
    "n = len(df_train_losses)  \n",
    "Nu = len(exc)\n",
    "xi = gpd_fit[\"xi\"]\n",
    "beta = gpd_fit[\"beta\"]\n",
    "\n",
    "VaR99_loss = var_pot_gpd(alpha=0.99, u=u_star, xi=xi, beta=beta, Nu=Nu, n=n)\n",
    "print(\"VaR 99% (pertes) =\", VaR99_loss)\n",
    "\n",
    "# Seuil en rendements (optionnel)\n",
    "VaR99_return = -VaR99_loss\n",
    "print(\"Seuil VaR 99% en rendements =\", VaR99_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
